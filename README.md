
# Real-Time Job Analytics Pipeline

## ğŸ“Œ Overview

This project demonstrates a **real-time data analytics pipeline** that ingests live events from a public API, streams them through AWS services, and prepares the data for large-scale analytics processing.

The goal of this project is to simulate **real-world data engineering use cases**, such as:

- Real-time ingestion  
- Event-driven architecture  
- Scalable streaming pipelines  
- Cloud-native design  

This project is designed to be **resume-ready and interview-ready**.

---

## ğŸ—ï¸ System Architecture

```
GitHub Events API
        |
        | (Continuous Polling)
        v
Python Producer (Microservice)
        |
        | (Streaming Events)
        v
AWS Kinesis Data Stream
        |
        | (Next Phase)
        v
Firehose â†’ AWS S3 (Raw / Bronze)
        |
        v
Azure Databricks (Structured Streaming)
```

---

## ğŸš€ Key Features

- Near real-time data ingestion  
- REST API â†’ Streaming conversion  
- Event-driven pipeline using AWS Kinesis  
- Fault-tolerant and scalable design  
- Secure IAM-based access  
- Production-grade logging  
- Defensive schema handling  

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|------|------------|
| Language | Python |
| API Source | GitHub Events API |
| Streaming | AWS Kinesis Data Streams |
| Cloud | AWS |
| Security | AWS IAM |
| Logging | Python logging |
| Version Control | Git & GitHub |

---

## ğŸ“‚ Project Structure

```
real-time-job-analytics/
â”‚
â”œâ”€â”€ data-producer/
â”‚   â””â”€â”€ github_events_producer.py
â”‚
â”œâ”€â”€ README.md
```

---

## ğŸ”‘ Prerequisites

Before running this project, ensure you have:

- Python 3.8+  
- AWS account  
- AWS CLI installed  
- Git installed  
- IAM user with required permissions  
- Active AWS Kinesis Data Stream  

---

## ğŸ” IAM Permissions

The Python producer uses a **least-privilege IAM policy**.

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "kinesis:PutRecord",
        "kinesis:PutRecords",
        "kinesis:DescribeStream",
        "kinesis:DescribeStreamSummary"
      ],
      "Resource": "*"
    }
  ]
}
```

---

## âš™ï¸ AWS Setup

### 1ï¸âƒ£ Create Kinesis Data Stream

- Stream Name: `job-analytics-stream`
- Region: `ap-south-1`
- Capacity Mode: On-demand

### 2ï¸âƒ£ Configure AWS Credentials Locally

```bash
aws configure
```

Verify configuration:

```bash
aws sts get-caller-identity
```

---

## ğŸ Python Environment Setup

Install required dependencies:

```bash
pip install boto3 requests
```

---

## â–¶ï¸ Running the Application

Navigate to the producer directory:

```bash
cd data-producer
python github_events_producer.py
```

### Sample Output

```
INFO GitHub Events Producer started
INFO Sent 30 events to Kinesis
INFO Sent 30 events to Kinesis
```

---

## ğŸ“Š Data Validation in Kinesis

To verify ingestion:

1. Open AWS Console  
2. Navigate to **Kinesis Data Streams**  
3. Select `job-analytics-stream`  
4. Open **Monitoring** tab  
5. Check:
   - IncomingRecords  
   - IncomingBytes  

Optional:
- Use **Data Viewer**
- Shard â†’ `ShardId-000000000000`
- Start Position â†’ `Latest`

---

## ğŸ§  Design Decisions

### ğŸ”¹ API to Streaming Conversion
The GitHub Events API is request-based.  
A continuously running Python service converts API responses into streaming events.

### ğŸ”¹ Partition Key Strategy

```python
PartitionKey = event.get("event_type", "UNKNOWN")
```

- Maintains ordering per event type  
- Prevents pipeline failure on missing fields  
- Enables scalable shard distribution  

### ğŸ”¹ Defensive Data Handling

- Uses `.get()` for schema tolerance  
- Prevents pipeline crashes  
- Captures malformed records for analysis  

### ğŸ”¹ Timezone-Aware Timestamps

All timestamps are stored in **UTC** using timezone-aware datetime objects.

---

## ğŸªµ Logging Strategy

The application uses Pythonâ€™s `logging` module instead of `print()`:

- Structured logs  
- Severity levels  
- Better observability  

Example:

```
2025-01-05 11:10:25 INFO Sent 30 events to Kinesis
```

---

## ğŸ”® Future Enhancements

- Kinesis Firehose â†’ S3 (Raw / Bronze layer)  
- Databricks Structured Streaming  
- Delta Lake tables  
- Data quality checks  
- Alerting & monitoring  
- Dashboarding  

---

## ğŸ¯ Interview Relevance

This project demonstrates:

- Real-time data ingestion  
- Streaming architecture  
- Cloud-native data engineering  
- IAM security best practices  
- Production-ready Python coding  

---

## âœ… Conclusion

This project simulates a **real-world real-time analytics pipeline** and serves as a strong foundation for advanced big data processing using **Databricks** and **Delta Lake**.
